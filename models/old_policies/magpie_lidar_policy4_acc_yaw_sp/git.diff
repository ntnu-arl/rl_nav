diff --git a/aerial_gym/config/env_config/env_with_obstacles.py b/aerial_gym/config/env_config/env_with_obstacles.py
index 44a3fe5..dd48d81 100644
--- a/aerial_gym/config/env_config/env_with_obstacles.py
+++ b/aerial_gym/config/env_config/env_with_obstacles.py
@@ -42,9 +42,9 @@ class EnvWithObstaclesCfg:
 
         use_warp = True
         lower_bound_min = [-7.50, -7.50, -5.0]  # lower bound for the environment space
-        lower_bound_max = [-5.0, -5.0, -3.0]  # lower bound for the environment space
-        upper_bound_min = [5.0, 5.0, 3.0]  # upper bound for the environment space
-        upper_bound_max = [7.5, 7.5, 5.0]  # upper bound for the environment space
+        lower_bound_max = [-5.0, -5.0, -5.0]  # lower bound for the environment space
+        upper_bound_min = [5.0, 5.0, 5.0]  # upper bound for the environment space
+        upper_bound_max = [7.50, 7.50, 5.0]  # upper bound for the environment space
 
     class env_config:
         include_asset_type = {
diff --git a/aerial_gym/config/task_config/lidar_navigation_task_config.py b/aerial_gym/config/task_config/lidar_navigation_task_config.py
index 4644868..77e6294 100644
--- a/aerial_gym/config/task_config/lidar_navigation_task_config.py
+++ b/aerial_gym/config/task_config/lidar_navigation_task_config.py
@@ -27,11 +27,13 @@ class task_config:
     target_max_ratio = [0.92, 0.80, 0.80]  # target ratio w.r.t environment bounds in x,y,z
 
     reward_parameters = {
-        "pos_reward_magnitude": 3.0,
+        "pos_reward_magnitude": 5.0,
         "pos_reward_exponent": 1.0,
-        "very_close_to_goal_reward_magnitude": 5.0,
+        "very_close_to_goal_reward_magnitude": 10.0,
         "very_close_to_goal_reward_exponent": 10.0,
-        "vel_direction_component_reward_magnitude": 2.0,
+        "vel_direction_component_reward_magnitude": 1.0,
+        "yaw_error_reward_magnitude": 0.0,
+        "yaw_error_reward_exponent": 1.0,
         "x_action_diff_penalty_magnitude": 0.3,
         "x_action_diff_penalty_exponent": 5.0,
         "y_action_diff_penalty_magnitude": 0.3,
@@ -48,7 +50,7 @@ class task_config:
         "z_absolute_action_penalty_exponent": 1.0,
         "yawrate_absolute_action_penalty_magnitude": 1.5,
         "yawrate_absolute_action_penalty_exponent": 2.0,
-        "collision_penalty": -10.0,
+        "collision_penalty": -50.0,
     }
 
     class vae_config:
@@ -64,9 +66,9 @@ class task_config:
         return_sampled_latent = True
 
     class curriculum:
-        min_level = 25
+        min_level = 10
         max_level = 70
-        check_after_log_instances = 2048
+        check_after_log_instances = 1024
         increase_step = 2
         decrease_step = 1
         success_rate_for_increase = 0.7
diff --git a/aerial_gym/env_manager/env_manager.py b/aerial_gym/env_manager/env_manager.py
index 919f935..28c42a1 100644
--- a/aerial_gym/env_manager/env_manager.py
+++ b/aerial_gym/env_manager/env_manager.py
@@ -281,6 +281,15 @@ class EnvManager(BaseManager):
         # logger.debug(f"Resetting environments {env_ids}.")
         self.IGE_env.reset_idx(env_ids)
         self.asset_manager.reset_idx(env_ids, self.global_tensor_dict["num_obstacles_in_env"])
+
+        # with a probability of 0.15 do a bernoulli trial to select half the obstacles to be removed from the environment
+        indices_with_half_obstacles = torch.bernoulli(
+            0.15 * torch.ones((len(env_ids)), device=self.device
+        )).nonzero(as_tuple=False).squeeze(-1)
+        lower_obs_indices = env_ids[indices_with_half_obstacles]
+        lower_num_obstacles = self.global_tensor_dict["num_obstacles_in_env"] // 2
+        self.asset_manager.reset_idx(lower_obs_indices, lower_num_obstacles)
+
         if self.cfg.env.use_warp:
             self.warp_env.reset_idx(env_ids)
         self.robot_manager.reset_idx(env_ids)
diff --git a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
index d96f633..ba4cffb 100644
--- a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
+++ b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
@@ -65,6 +65,10 @@ class LiDARNavigationTask(BaseTask):
             (self.sim_env.num_envs, 3), device=self.device, requires_grad=False
         )
 
+        self.target_yaw = torch.zeros(
+            (self.sim_env.num_envs,), device=self.device, requires_grad=False
+        )
+
         self.target_min_ratio = torch.tensor(
             self.task_config.target_min_ratio, device=self.device, requires_grad=False
         ).expand(self.sim_env.num_envs, -1)
@@ -165,6 +169,10 @@ class LiDARNavigationTask(BaseTask):
             max=self.obs_dict["env_bounds_max"][env_ids],
             ratio=target_ratio[env_ids],
         )
+        self.target_yaw[env_ids] = torch_rand_float_tensor(
+            -torch.pi*torch.ones_like(env_ids, device=self.device),
+            torch.pi*torch.ones_like(env_ids, device=self.device),
+        )
         self.obs_dict["robot_prev_actions"][env_ids] = 0.0
         
         # logger.warning(f"reset envs: {env_ids}")
@@ -421,7 +429,9 @@ class LiDARNavigationTask(BaseTask):
             (torch.rand_like(euler_angles) - 0.5)
         self.task_obs["observations"][:, 4] = perturbed_euler_angles[:, 0]
         self.task_obs["observations"][:, 5] = perturbed_euler_angles[:, 1]
-        self.task_obs["observations"][:, 6] = 0.0
+        self.task_obs["observations"][:, 6] = ssa(
+            self.target_yaw - euler_angles[:, 2]
+        )
         self.task_obs["observations"][:,
                                       7:10] = self.obs_dict["robot_body_linvel"]
         self.task_obs["observations"][:,
@@ -442,10 +452,15 @@ class LiDARNavigationTask(BaseTask):
         self.pos_error_vehicle_frame[:] = quat_rotate_inverse(
             robot_vehicle_orientation, (target_position - robot_position)
         )
+        yaw_error = ssa(
+            self.target_yaw - obs_dict["robot_euler_angles"][:, 2]
+        )
+
         return compute_reward(
             self.pos_error_vehicle_frame,
             self.pos_error_vehicle_frame_prev,
             self.obs_dict["robot_vehicle_linvel"],
+            yaw_error,
             obs_dict["crashes"],
             obs_dict["robot_actions"],
             obs_dict["robot_prev_actions"],
@@ -476,6 +491,7 @@ def compute_reward(
     pos_error,
     prev_pos_error,
     robot_vehicle_linvel,
+    yaw_error,
     crashes,
     action,
     prev_action,
@@ -483,10 +499,9 @@ def compute_reward(
     curriculum_progress_fraction,
     parameter_dict,
 ):
-    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
+    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
     MULTIPLICATION_FACTOR_REWARD = 1.0 + (2.0) * curriculum_progress_fraction
     dist = torch.norm(pos_error, dim=1)
-    prev_dist_to_goal = torch.norm(prev_pos_error, dim=1)
     pos_reward = exponential_reward_function(
         parameter_dict["pos_reward_magnitude"],
         parameter_dict["pos_reward_exponent"],
@@ -504,15 +519,21 @@ def compute_reward(
     unit_vec_to_goal = pos_error / (dist.unsqueeze(1) + 1e-6)
     vel_dir_component = torch.sum(robot_vel_dir * unit_vec_to_goal, dim=1)
     vel_dir_component_reward = torch.where(vel_dir_component > 0,
-                                           parameter_dict["vel_direction_component_reward_magnitude"] * vel_dir_component,
+                                           torch.min(robot_vel_norm, 2.0* torch.ones_like(vel_dir_component)) * vel_dir_component,
                                            -0.2 * torch.ones_like(vel_dir_component)
-                                           ) * torch.min(dist , torch.ones_like(dist))
-    
+                                           ) * torch.min((dist/2)**2 , torch.ones_like(dist))
+
+    yaw_error_reward = exponential_reward_function(
+        parameter_dict["yaw_error_reward_magnitude"],
+        parameter_dict["yaw_error_reward_exponent"],
+        yaw_error,
+    )
+
     # for acceleration setpoint task
     # any velocity greater than 3m/s be penalized
     vel_magnitude_penalty = exponential_penalty_function(
         2.0,
-        2.0,
+        5.0,
         torch.clamp(robot_vel_norm - 3.0, min=0.0),
     )
 
@@ -525,13 +546,13 @@ def compute_reward(
     vel_penalty_for_acc = vel_magnitude_penalty + negative_x_vel_penalty
 
     # stable at goal reward 
-    stable_at_goal_reward = exponential_reward_function(
-        2.0,
-        10.0,
+    vel_norm_reward = exponential_reward_function(
+        3.0,
+        5.0,
         robot_vel_norm,
-    ) * very_close_to_goal_reward
+    )
+    stable_at_goal_reward = very_close_to_goal_reward * (1 + vel_norm_reward + yaw_error_reward)
 
-    distance_from_goal_reward = (20.0 - dist) / 20.0
     action_diff = action - prev_action
     x_diff_penalty = exponential_penalty_function(
         parameter_dict["x_action_diff_penalty_magnitude"],
@@ -560,6 +581,11 @@ def compute_reward(
         parameter_dict["x_absolute_action_penalty_exponent"],
         action[:, 0],
     )
+    y_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
+        parameter_dict["y_absolute_action_penalty_magnitude"],
+        parameter_dict["y_absolute_action_penalty_exponent"],
+        action[:, 1],
+    )
     z_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
         parameter_dict["z_absolute_action_penalty_magnitude"],
         parameter_dict["z_absolute_action_penalty_exponent"],
@@ -570,11 +596,6 @@ def compute_reward(
         parameter_dict["yawrate_absolute_action_penalty_exponent"],
         action[:, 3],
     )
-    y_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
-        parameter_dict["y_absolute_action_penalty_magnitude"],
-        parameter_dict["y_absolute_action_penalty_exponent"],
-        action[:, 1],
-    )
     absolute_action_penalty = x_absolute_penalty + \
         z_absolute_penalty + yawrate_absolute_penalty + y_absolute_penalty
     total_action_penalty = action_diff_penalty + absolute_action_penalty
@@ -584,23 +605,13 @@ def compute_reward(
         MULTIPLICATION_FACTOR_REWARD
         * (
             pos_reward
-            + very_close_to_goal_reward
-            # + getting_closer_reward
             + vel_dir_component_reward
-            + distance_from_goal_reward
-            # + lidar_data_penalty
             + stable_at_goal_reward
             + vel_penalty_for_acc
             + total_action_penalty
         )
     )
 
-    # reward[:] = torch.where(
-    #     lidar_data_penalty < -2.0,
-    #     lidar_data_penalty,
-    #     reward
-    # )
-
     reward[:] = torch.where(
         crashes > 0,
         parameter_dict["collision_penalty"] * torch.ones_like(reward),
