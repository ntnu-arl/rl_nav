diff --git a/aerial_gym/config/task_config/lidar_navigation_task_config.py b/aerial_gym/config/task_config/lidar_navigation_task_config.py
index 4644868..d5dc3bf 100644
--- a/aerial_gym/config/task_config/lidar_navigation_task_config.py
+++ b/aerial_gym/config/task_config/lidar_navigation_task_config.py
@@ -30,15 +30,17 @@ class task_config:
         "pos_reward_magnitude": 3.0,
         "pos_reward_exponent": 1.0,
         "very_close_to_goal_reward_magnitude": 5.0,
-        "very_close_to_goal_reward_exponent": 10.0,
+        "very_close_to_goal_reward_exponent": 5.0,
+        "yaw_error_reward_magnitude": 3.0,
+        "yaw_error_reward_exponent": 1.0,
         "vel_direction_component_reward_magnitude": 2.0,
-        "x_action_diff_penalty_magnitude": 0.3,
+        "x_action_diff_penalty_magnitude": 0.15,
         "x_action_diff_penalty_exponent": 5.0,
-        "y_action_diff_penalty_magnitude": 0.3,
+        "y_action_diff_penalty_magnitude": 0.15,
         "y_action_diff_penalty_exponent": 5.0,
-        "z_action_diff_penalty_magnitude": 0.3,
+        "z_action_diff_penalty_magnitude": 0.15,
         "z_action_diff_penalty_exponent": 5.0,
-        "yawrate_action_diff_penalty_magnitude": 0.3,
+        "yawrate_action_diff_penalty_magnitude": 0.15,
         "yawrate_action_diff_penalty_exponent": 5.0,
         "x_absolute_action_penalty_magnitude": 0.1,
         "x_absolute_action_penalty_exponent": 0.3,
diff --git a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
index d96f633..595f6ae 100644
--- a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
+++ b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
@@ -65,6 +65,10 @@ class LiDARNavigationTask(BaseTask):
             (self.sim_env.num_envs, 3), device=self.device, requires_grad=False
         )
 
+        self.target_yaw = torch.zeros(
+            (self.sim_env.num_envs,), device=self.device, requires_grad=False
+        )
+
         self.target_min_ratio = torch.tensor(
             self.task_config.target_min_ratio, device=self.device, requires_grad=False
         ).expand(self.sim_env.num_envs, -1)
@@ -165,6 +169,10 @@ class LiDARNavigationTask(BaseTask):
             max=self.obs_dict["env_bounds_max"][env_ids],
             ratio=target_ratio[env_ids],
         )
+        self.target_yaw[env_ids] = torch_rand_float_tensor(
+            -torch.pi*torch.ones_like(env_ids, device=self.device),
+            torch.pi*torch.ones_like(env_ids, device=self.device),
+        )
         self.obs_dict["robot_prev_actions"][env_ids] = 0.0
         
         # logger.warning(f"reset envs: {env_ids}")
@@ -421,7 +429,9 @@ class LiDARNavigationTask(BaseTask):
             (torch.rand_like(euler_angles) - 0.5)
         self.task_obs["observations"][:, 4] = perturbed_euler_angles[:, 0]
         self.task_obs["observations"][:, 5] = perturbed_euler_angles[:, 1]
-        self.task_obs["observations"][:, 6] = 0.0
+        self.task_obs["observations"][:, 6] = ssa(
+            self.target_yaw - euler_angles[:, 2]
+        )
         self.task_obs["observations"][:,
                                       7:10] = self.obs_dict["robot_body_linvel"]
         self.task_obs["observations"][:,
@@ -442,10 +452,15 @@ class LiDARNavigationTask(BaseTask):
         self.pos_error_vehicle_frame[:] = quat_rotate_inverse(
             robot_vehicle_orientation, (target_position - robot_position)
         )
+        yaw_error = ssa(
+            self.target_yaw - obs_dict["robot_euler_angles"][:, 2]
+        )
+
         return compute_reward(
             self.pos_error_vehicle_frame,
             self.pos_error_vehicle_frame_prev,
             self.obs_dict["robot_vehicle_linvel"],
+            yaw_error,
             obs_dict["crashes"],
             obs_dict["robot_actions"],
             obs_dict["robot_prev_actions"],
@@ -476,6 +491,7 @@ def compute_reward(
     pos_error,
     prev_pos_error,
     robot_vehicle_linvel,
+    yaw_error,
     crashes,
     action,
     prev_action,
@@ -483,7 +499,7 @@ def compute_reward(
     curriculum_progress_fraction,
     parameter_dict,
 ):
-    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
+    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
     MULTIPLICATION_FACTOR_REWARD = 1.0 + (2.0) * curriculum_progress_fraction
     dist = torch.norm(pos_error, dim=1)
     prev_dist_to_goal = torch.norm(prev_pos_error, dim=1)
@@ -506,8 +522,14 @@ def compute_reward(
     vel_dir_component_reward = torch.where(vel_dir_component > 0,
                                            parameter_dict["vel_direction_component_reward_magnitude"] * vel_dir_component,
                                            -0.2 * torch.ones_like(vel_dir_component)
-                                           ) * torch.min(dist , torch.ones_like(dist))
-    
+                                           ) * torch.min(dist / 2.0 , torch.ones_like(dist))
+
+    yaw_error_reward = exponential_reward_function(
+        parameter_dict["yaw_error_reward_magnitude"],
+        parameter_dict["yaw_error_reward_exponent"],
+        yaw_error,
+    )
+
     # for acceleration setpoint task
     # any velocity greater than 3m/s be penalized
     vel_magnitude_penalty = exponential_penalty_function(
@@ -518,18 +540,19 @@ def compute_reward(
 
     negative_x_vel_penalty = exponential_penalty_function(
         2.0,
-        8.0,
+        5.0,
         torch.clamp(robot_vehicle_linvel[:, 0], min=0.0),
     )
 
     vel_penalty_for_acc = vel_magnitude_penalty + negative_x_vel_penalty
 
     # stable at goal reward 
-    stable_at_goal_reward = exponential_reward_function(
+    vel_norm_reward = exponential_reward_function(
         2.0,
         10.0,
         robot_vel_norm,
-    ) * very_close_to_goal_reward
+    )
+    stable_at_goal_reward = very_close_to_goal_reward * (vel_norm_reward + yaw_error_reward)
 
     distance_from_goal_reward = (20.0 - dist) / 20.0
     action_diff = action - prev_action
@@ -560,6 +583,11 @@ def compute_reward(
         parameter_dict["x_absolute_action_penalty_exponent"],
         action[:, 0],
     )
+    y_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
+        parameter_dict["y_absolute_action_penalty_magnitude"],
+        parameter_dict["y_absolute_action_penalty_exponent"],
+        action[:, 1],
+    )
     z_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
         parameter_dict["z_absolute_action_penalty_magnitude"],
         parameter_dict["z_absolute_action_penalty_exponent"],
@@ -570,11 +598,6 @@ def compute_reward(
         parameter_dict["yawrate_absolute_action_penalty_exponent"],
         action[:, 3],
     )
-    y_absolute_penalty = curriculum_progress_fraction * exponential_penalty_function(
-        parameter_dict["y_absolute_action_penalty_magnitude"],
-        parameter_dict["y_absolute_action_penalty_exponent"],
-        action[:, 1],
-    )
     absolute_action_penalty = x_absolute_penalty + \
         z_absolute_penalty + yawrate_absolute_penalty + y_absolute_penalty
     total_action_penalty = action_diff_penalty + absolute_action_penalty
