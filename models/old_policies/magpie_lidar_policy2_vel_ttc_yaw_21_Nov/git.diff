diff --git a/aerial_gym/config/task_config/lidar_navigation_task_config.py b/aerial_gym/config/task_config/lidar_navigation_task_config.py
index fae6a35..53bed88 100644
--- a/aerial_gym/config/task_config/lidar_navigation_task_config.py
+++ b/aerial_gym/config/task_config/lidar_navigation_task_config.py
@@ -7,7 +7,8 @@ class task_config:
     sim_name = "base_sim"
     env_name = "env_with_obstacles"
     robot_name = "magpie"
-    controller_name = "lmf2_acceleration_control"
+    # controller_name = "lmf2_acceleration_control"
+    controller_name = "lmf2_velocity_control"
     args = {}
     num_envs = 512
     use_warp = True
@@ -30,7 +31,7 @@ class task_config:
         "pos_reward_magnitude": 3.0,
         "pos_reward_exponent": 1.0,
         "very_close_to_goal_reward_magnitude": 5.0,
-        "very_close_to_goal_reward_exponent": 10.0,
+        "very_close_to_goal_reward_exponent": 7.0,
         "vel_direction_component_reward_magnitude": 1.0,
         "x_action_diff_penalty_magnitude": 0.1,
         "x_action_diff_penalty_exponent": 5.0,
@@ -81,20 +82,6 @@ class task_config:
 
 
 
-    # def action_transformation_function(action):
-    #     clamped_action = torch.clamp(action, -1.0, 1.0)
-    #     max_yawrate = torch.pi / 3  # [rad/s]
-
-    #     processed_action = torch.zeros(
-    #         (clamped_action.shape[0], 4), device=task_config.device, requires_grad=False
-    #     )
-
-    #     processed_action[:, 0] = -(clamped_action[:, 0]+1.0)
-    #     processed_action[:, 1] = clamped_action[:, 1]
-    #     processed_action[:, 2] = clamped_action[:, 2]
-    #     processed_action[:, 3] = clamped_action[:, 3] * max_yawrate
-    #     return processed_action
-
     def action_transformation_function(action):
         clamped_action = torch.clamp(action, -1.0, 1.0)
         max_yawrate = torch.pi / 3  # [rad/s]
@@ -103,8 +90,22 @@ class task_config:
             (clamped_action.shape[0], 4), device=task_config.device, requires_grad=False
         )
 
-        processed_action[:, 0:3] = 2 * (clamped_action[:, 0:3])
-        # processed_action[:, 1] = clamped_action[:, 1]
-        # processed_action[:, 2] = clamped_action[:, 2]
+        processed_action[:, 0] = -(clamped_action[:, 0]+1.0)
+        processed_action[:, 1] = clamped_action[:, 1]
+        processed_action[:, 2] = clamped_action[:, 2]
         processed_action[:, 3] = clamped_action[:, 3] * max_yawrate
         return processed_action
+
+    # def action_transformation_function(action):
+    #     clamped_action = torch.clamp(action, -1.0, 1.0)
+    #     max_yawrate = torch.pi / 3  # [rad/s]
+
+    #     processed_action = torch.zeros(
+    #         (clamped_action.shape[0], 4), device=task_config.device, requires_grad=False
+    #     )
+
+    #     processed_action[:, 0:3] = 2 * (clamped_action[:, 0:3])
+    #     # processed_action[:, 1] = clamped_action[:, 1]
+    #     # processed_action[:, 2] = clamped_action[:, 2]
+    #     processed_action[:, 3] = clamped_action[:, 3] * max_yawrate
+    #     return processed_action
diff --git a/aerial_gym/env_manager/env_manager.py b/aerial_gym/env_manager/env_manager.py
index 919f935..0acb5bd 100644
--- a/aerial_gym/env_manager/env_manager.py
+++ b/aerial_gym/env_manager/env_manager.py
@@ -280,7 +280,18 @@ class EnvManager(BaseManager):
         # finally reset the robot manager that resets the robot state tensors and the sensors
         # logger.debug(f"Resetting environments {env_ids}.")
         self.IGE_env.reset_idx(env_ids)
-        self.asset_manager.reset_idx(env_ids, self.global_tensor_dict["num_obstacles_in_env"])
+        num_obstacles = self.global_tensor_dict["num_obstacles_in_env"]
+        self.asset_manager.reset_idx(env_ids, num_obstacles)
+        nk = self.asset_manager.num_keep_in_env
+        self.asset_manager.num_keep_in_env = self.asset_manager.num_keep_in_env // 2
+        
+        # Do a bernoulli sampling across indices such that each has a probability of 0.15 to be 1
+        samples = torch.bernoulli(0.15*torch.ones(len(env_ids), device=self.device))
+        selected_indices = torch.nonzero(samples)
+
+        self.asset_manager.reset_idx(env_ids[selected_indices], num_obstacles//2)
+        self.asset_manager.num_keep_in_env = nk
+
         if self.cfg.env.use_warp:
             self.warp_env.reset_idx(env_ids)
         self.robot_manager.reset_idx(env_ids)
diff --git a/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_aerialgym.py b/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_aerialgym.py
index e948137..dcc3edc 100644
--- a/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_aerialgym.py
+++ b/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_aerialgym.py
@@ -216,8 +216,8 @@ env_configs = dict(
         max_grad_norm=1.0,
         num_batches_per_epoch=4,
         exploration_loss_coeff=0.001,
-        with_wandb=False,
-        wandb_project="quad",
+        with_wandb=True,
+        wandb_project="lidar_nav_task",
         wandb_user="mihirkulkarni",
     ),
 
diff --git a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
index ea0e705..9b5ede6 100644
--- a/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
+++ b/aerial_gym/task/lidar_navigation_task/lidar_navigation_task.py
@@ -85,6 +85,7 @@ class LiDARNavigationTask(BaseTask):
         )
 
         self.time_to_collision = torch.zeros((self.sim_env.num_envs), device=self.device)
+        self.target_yaw = torch.zeros((self.sim_env.num_envs), device=self.device)
 
         # Get the dictionary once from the environment and use it to get the observations later.
         # This is to avoid constant retuning of data back anf forth across functions as the tensors update and can be read in-place.
@@ -166,7 +167,12 @@ class LiDARNavigationTask(BaseTask):
             ratio=target_ratio[env_ids],
         )
         self.obs_dict["robot_prev_actions"][env_ids] = 0.0
-        
+
+        self.target_yaw[env_ids] = torch_rand_float_tensor(
+            -torch.pi * torch.ones(len(env_ids), device=self.device),
+            torch.pi * torch.ones(len(env_ids), device=self.device),
+        )
+
         # logger.warning(f"reset envs: {env_ids}")
         self.infos = {}
         return
@@ -278,14 +284,14 @@ class LiDARNavigationTask(BaseTask):
         pointcloud_obs = self.obs_dict["depth_range_pixels"].squeeze(1)
         self.world_dir_vectors[:] = pointcloud_obs - self.obs_dict["robot_position"].unsqueeze(1).unsqueeze(1)
         range_obs = torch.norm(self.world_dir_vectors, dim=-1)
+        range_obs_flat = range_obs.view(self.num_envs, -1)
+        self.world_unit_dir = self.world_dir_vectors.view(self.num_envs, -1, 3) / (range_obs_flat.unsqueeze(-1) + 1e-6)
+        
         range_obs[range_obs > 10] = 10.0
         range_obs[range_obs < 0.2] = 10.0
 
         image_obs = range_obs.clone()
 
-        range_obs_flat = range_obs.view(self.num_envs, -1)
-
-        self.world_unit_dir = self.world_dir_vectors.view(self.num_envs, -1, 3) / (range_obs_flat.unsqueeze(-1) + 1e-6)
 
         self.world_dir_vectors_flat = self.world_dir_vectors.view(self.num_envs, -1, 3)
         self.vel_component_along_dir = torch.sum(
@@ -294,20 +300,26 @@ class LiDARNavigationTask(BaseTask):
 
         time_to_collision = torch.where(
             self.vel_component_along_dir > 0,
-            (range_obs_flat - 0.2) / (self.vel_component_along_dir + 1e-6),
+            (range_obs_flat) / (self.vel_component_along_dir + 1e-6),
             10.0 * torch.ones_like(range_obs_flat)
         )
 
-        self.time_to_collision[:] = torch.clamp(torch.min(time_to_collision, dim=-1).values, 0.0, 10.0)
+        # print(self.vel_component_along_dir.shape,range_obs_flat.shape)
 
+        self.time_to_collision[:] = torch.clamp(torch.min(time_to_collision, dim=-1).values, 0.0, 10.0)
+        min_indices = torch.min(time_to_collision, dim=-1).indices
+        # print(min_indices)
+        # print("time to collision: ", self.time_to_collision[0])
+        # print(range_obs_flat[0, min_indices[0]], self.vel_component_along_dir[0, min_indices[0]])
+        
         inv_range_image = 1 / image_obs
 
         # invalid pixels are set to max distance
         # # downsample the image using max pooling
-        image_obs_ds = torch.nn.functional.max_pool2d(
+        inv_image_obs_ds = torch.nn.functional.max_pool2d(
             inv_range_image.unsqueeze(1), (3, 6)).squeeze(1)
 
-        self.downsampled_lidar_data[:] = image_obs_ds.reshape(
+        self.downsampled_lidar_data[:] = inv_image_obs_ds.reshape(
             (self.num_envs, -1)).to(self.device)
         return
 
@@ -431,7 +443,9 @@ class LiDARNavigationTask(BaseTask):
             (torch.rand_like(euler_angles) - 0.5)
         self.task_obs["observations"][:, 4] = perturbed_euler_angles[:, 0]
         self.task_obs["observations"][:, 5] = perturbed_euler_angles[:, 1]
-        self.task_obs["observations"][:, 6] = 0.0
+        self.task_obs["observations"][:, 6] = ssa(
+            self.target_yaw - euler_angles[:, 2]
+        )
         self.task_obs["observations"][:,
                                       7:10] = self.obs_dict["robot_body_linvel"]
         self.task_obs["observations"][:,
@@ -452,10 +466,13 @@ class LiDARNavigationTask(BaseTask):
         self.pos_error_vehicle_frame[:] = quat_rotate_inverse(
             robot_vehicle_orientation, (target_position - robot_position)
         )
+        yaw_error = ssa(self.target_yaw - self.obs_dict["robot_euler_angles"][:, 2])
         return compute_reward(
             self.pos_error_vehicle_frame,
             self.pos_error_vehicle_frame_prev,
             self.obs_dict["robot_vehicle_linvel"],
+            self.obs_dict["robot_body_angvel"],
+            yaw_error,
             obs_dict["crashes"],
             obs_dict["robot_actions"],
             obs_dict["robot_prev_actions"],
@@ -486,6 +503,8 @@ def compute_reward(
     pos_error,
     prev_pos_error,
     robot_vehicle_linvel,
+    robot_body_angvel,
+    yaw_error,
     crashes,
     action,
     prev_action,
@@ -493,7 +512,7 @@ def compute_reward(
     curriculum_progress_fraction,
     parameter_dict,
 ):
-    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
+    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float, Dict[str, Tensor]) -> Tuple[Tensor, Tensor]
     MULTIPLICATION_FACTOR_REWARD = 1.0 + (2.0) * curriculum_progress_fraction
     dist = torch.norm(pos_error, dim=1)
     prev_dist_to_goal = torch.norm(prev_pos_error, dim=1)
@@ -513,7 +532,7 @@ def compute_reward(
         robot_vel_norm.unsqueeze(1) + 1e-6)
     unit_vec_to_goal = pos_error / (dist.unsqueeze(1) + 1e-6)
 
-    reasonable_vel_along_goal = exponential_reward_function(
+    reasonable_vel = exponential_reward_function(
         2.0,
         2.0,
         (robot_vel_norm - 2.0)
@@ -523,7 +542,7 @@ def compute_reward(
     vel_dir_component = torch.sum(robot_vel_dir * unit_vec_to_goal, dim=1)
 
     vel_dir_component_reward = torch.where(vel_dir_component > 0,
-                                           parameter_dict["vel_direction_component_reward_magnitude"] * vel_dir_component * reasonable_vel_along_goal,
+                                           parameter_dict["vel_direction_component_reward_magnitude"] * vel_dir_component * reasonable_vel,
                                            -0.2 * torch.ones_like(vel_dir_component)
                                            ) * torch.min((dist/3.0) , torch.ones_like(dist))
     
@@ -544,11 +563,33 @@ def compute_reward(
     vel_penalty_for_acc = vel_magnitude_penalty + negative_x_vel_penalty
 
     # stable at goal reward 
-    stable_at_goal_reward = exponential_reward_function(
+    low_vel_reward = exponential_reward_function(
         2.0,
-        10.0,
+        5.0,
         robot_vel_norm,
-    ) * very_close_to_goal_reward
+    )
+
+    correct_yaw_reward = exponential_reward_function(
+        3.0,
+        5.0,
+        yaw_error
+    )
+
+    low_angvel_reward = exponential_reward_function(
+        1.0,
+        5.0,
+        robot_body_angvel[:, 2],
+    )
+        
+
+    stable_at_goal_reward = torch.where(
+        dist < 1.0,
+        (low_vel_reward + correct_yaw_reward + low_angvel_reward),
+        torch.zeros_like(low_vel_reward),
+    )
+    
+
+
 
     distance_from_goal_reward = (20.0 - dist) / 20.0
     action_diff = action - prev_action
@@ -598,10 +639,10 @@ def compute_reward(
         z_absolute_penalty + yawrate_absolute_penalty + y_absolute_penalty
     total_action_penalty = action_diff_penalty + absolute_action_penalty
 
-    time_to_collision_penalty = exponential_penalty_function(
-        3.0,
-        1.0,
-        time_to_collision
+    time_to_collision_penalty = exponential_reward_function(
+        -3.0,
+        0.8,
+        time_to_collision**2
     )
 
     # combined reward
